\section{Implemented features}
This section describes the solution, that was handed in. The core rendering engine was written in C++, and a (minimal) UI was written in C\#. Also note that the team did not have any prior experience in writing C++.

\subsection{Overall architecture}
Our goal with the architecture was to create a renderer which was as maintainable as possible, such that new features could be added on top without creating problems with existing features. By creating UML-diagrams, applying basic object oriented design patterns we were able to create a simple and powerful architecture. \\

Unfortunately, since our scope for the solution was just a static rendition, performance was not a priority. Therefore, many architectural choices were not made with performance in mind. An example of a poor architecture in regards to performance, is the lack of a global convention on normalization of vectors. Since we do not know if a vector is already normalized, the system normalizes vectors whenever it needs them, which is wasted processing if the vector is already normalized. Furthermore, none of the proposed optimization-techniques such as kd-trees or interpolation were implemented.

\subsection{Objects}
We have implemented the three most basic mathematical objects; spheres, planes, and triangles. Furthermore triangles can be joined together in a mesh.\\
Triangles also stand out mathematically in that they are basically a clipped plane, and you have to take additional steps to make sure you don’t intersect “outside” the triangle. The sphere and plane however are straightforward, and you just have to worry about special cases like rays not intersecting a plane and intersections with a sphere can result in a single or two points.\\

Meshes include a slight optimization in the form of a bounding sphere, which is checked for intersection before checking any of the potentially many triangles in the mesh. The bounding sphere is calculated as the center position of a minimal bounding box around the mesh and the minimum radius required to enclose all vertices. \\

We wanted to import models from .obj files, but didn’t have time due to pressure from other courses. The ray tracer itself fully supports meshes, and could be improved much further by smoothing the triangles based on normals, or maybe doing some tessellation on them.

\subsection{Shaders}
Since different shaders are similar and requires mostly the same arguments to compute, an inheritance structure was established, in which shaders of different types derive from the same shader-base class. The shader base allowed us to have a cleaner code style and allows for an object to have an arbitrary amount of shaders on it. That way we could implement an ambient-, a diffuse- and a specular shader, and blend the resulting colors additively together. This allows for a large amount of combinations and implementing a new shader in the system would require very few if any changes to the existing code. \\

We would have like to added textures and maps to the shaders to allow distortions and more realistic surfaces. These would improve the look of all the other features we have added, but due to time constraints this proved itself impossible, and other tasks had higher priorities.

\subsection{Lights}
As all lights can be defined by a color, intensity and direction, we used an inheritance structure where lights of different types derive from a light-base class. This light base provides the methods needed to get the relevant information about the light. Then different implementations such as directional- and positional lights could be implemented. For a directional light, the method \texttt{GetDirectionOnPoint(Point3d point)}, returns the direction vector which the light is constructed with, but with a positional light, the method returns a new vector from the position of the light to the point given in the parameter.\\

To allow lights to have a color, during the shading the color of the light and the color of the shader is multiplied together, and the result is then divided by 255. By doing this a red light which shines on a white object will only make the object appear red.\\

Since positional lights looses intensity the further away from the light source you go, the method \texttt{GetIntensityOnPoint(Point3d point)} was added to the light base. Here, directional lights returns the intensity value they were constructed with and the positional light returns a value which is divided by the distance times a fall-off value.\\

In future versions of our solution we could implement spotlights, area lights, or we could have added a range to the light sources. 

\subsection{Shadows}
In ray tracing, shadows are generated by taking the intersection point and checking from collisions in the direction of the light. \\
Our solution provides a method \texttt{GetLightsThatHitPoint(Point3d hit)}, which returns the list of lights which are not intersected by objects in the scene. By iterating over each light source in the scene and then creating \texttt{Line3d objects}, called shadow rays, from we could iterate over the scene objects and check for intersections.This creates a solution which runs in $O(N \cdot L)$ for each intersection, where $N$ is the amount of objects in the scene and $L$ is the amount of light sources. \\

In real life shadows are often soft and not hard as the initial solution provides, therefore to improve on this method we added a function called \texttt{GetLightsThatHitPointSoftShadows(Point3d hit)}. To create soft shadows each light source generates a number of twisted shadow rays. These shadow rays’ direction are all distorted randomly on the $x$-,$y$- and $z$-axis by a small amount. Then for each of these twisted shadow rays which are intersected, the intensity of the lightsource decreases by 1/(the number of twisted shadow rays). This solution runs in $O(N \cdot L \cdot T)$ for each intersection where $N$ is the amount of objects in the scene, $L$ is the amount of light sources and $T$ is the amount of shadow rays. \\

The implementation of the hard shadows is both efficient and correct, and we believe it to be the best fast way to create hard shadows using ray tracing. The soft shadows on the other hand could use a lot of improvements. First of all since it is based on randomized numbers, its accuracy is low and only when high numbers of shadow rays are created the noise gets diminished. But again, while soft-shadows provides a more realistic-looking rendering, it comes at a price of both processing-time and memory-consumption, since a lot of objects need to be created for each intersection.

\subsection{Reflection}
Reflectiveness is defined by the material of an object. Our implementation of the algorithm starts when a ray hits a reflective object. The ray is reflected of the surface according to the law of reflection\footnote{http://www.physicsclassroom.com/class/refln/Lesson-1/The-Law-of-Reflection} stating that the angle of incident ray (relative to the normal of the surface) is equal to the angle of reflection, and continues in its new direction like usual. The resulting color of this new ray and all subsequent rays are blended together to make the final color, based on the material’s reflectivity.

\subsection{Refraction}
Our refraction-implementation is based on Snell’s law\footnote{\url{http://www.flipcode.com/archives/reflection_transmission.pdf}} and the Fresnel equations\footnote{\url{http://en.wikipedia.org/wiki/Fresnel_equations}}. We take into account the possibility of total internal reflection. To get us started, scene-objects has a refraction-index, a reflectiveness and a transparency. The whole scene has a refraction-index, too. When a ray intersects the boundary of an object, we determine whether there’s a contribution from refraction and reflection, and then, if relevant, we determine each of these contributions separately and finally blend these contributions together. Notice, that determining either of these contributions usually implies several additional recursive calls to the same algorithm. \\

Our solution has some flaws, though. We wanted to provide room for an object being shaded in its own color, as opposed to shade (a point on) it only using what reflects and refracts from that point; this approach doesn’t would not display the material itself. The “correct” solution is to (aside the aforementioned reflection and refraction) also take into account, what colours the material (of the object) will absorb, and shade it as a mix of these three contributions. Our solution forces a shading-contribution onto a material, and we then decide in the following in what amount we should blend in the refraction- and reflection-contributions.  \\

We had a hard time evaluating if our solution was correct. This is due to two facts. Ray-tracers are hard to debug, because you do not know which pixel or where in the scene you are currently debugging, while going over the millions of pixels that is to be generated. Secondly, we had a hard time imagining what a 4 meter radius sphere would actually look like in real life if it was made out of glass. Especially, around total internal reflection we did not know what it would / should look like in real life.